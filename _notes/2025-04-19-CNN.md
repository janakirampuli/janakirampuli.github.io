---
layout: notes
title: "Convolutional Neural Networks"
tags: [ML, AI]
date: 2025-04-19
---

## What is CNN?

Convolutional Neural Networks (CNNs) are a specialized class of deep learning models designed for processing grid-structured data like images, leveraging spatial relationships through hierarchical feature learning. Unlike traditional neural networks that flatten input data, CNNs preserve spatial topology by analyzing local regions through learned filters. This architecture enables automatic extraction of features ranging from simple edges to complex object representations without manual feature engineering

---

## The Architecture of CNNs

Now let's think what is wrong with the feed forward neural networks that there is a need for new architecture? A computer sees an image as a matrix of numbers with ($rows*columns*number$ of channels) shape. Any real-world image would be at least 200*200*3 pixels. Now if we flatten out the image into a 1D matrix the input layer itself should have 1,20,000 neurons. Dealing with such a huge amount of parameters requires many neurons and it may lead to overfitting. 

Now what CNN does is it looks at a patch of an image at a time and move forward in this manner to derive complete information. It involves very few neurons with fewer parameters to scan an entire image to learn essential features.

---

## Layers of CNN

### Input layer

The input layer accepts raw pixel values from an image, typically represented as a three-dimensional tensor (height, width, depth). For example, a color image with dimensions $100×100$ pixels would have a shape of $100×100×3$, where 3 represents the RGB color channels

### Convolution layers

Convolutional layers are the core building blocks of CNNs. These layers apply filters (also known as kernels) that slide over the input image to detect specific features such as edges, textures, and shapes. Each filter produces a feature map that highlights patterns in the input data. The early layers typically detect simple features like edges and corners, while deeper layers identify more complex patterns like objects and faces. This hierarchical learning approach is one of the key strengths of CNNs.

### Padding and Stride

Two important parameters in the convolution operation are padding and stride:

- Padding: Adding extra pixels (usually zeros) around the border of the input image to control the spatial dimensions of the output feature map

- Stride: The step size with which the kernel moves across the input image

<p style="text-align: center;">
    <img src="/assets/images/2_conv.png" alt="CNN architecture" width="60%" class="img-center">
</p>


### Activation functions

After convolution, an activation function is applied to introduce non-linearity into the model. The Rectified Linear Unit (ReLU) is commonly used due to its effectiveness in accelerating convergence during training. ReLU works by replacing all negative values with zero, allowing for faster and more effective training of deep neural networks.

### Pooling layers

Pooling layers reduce the spatial dimensions of the feature maps while preserving their essential information. This downsampling process serves several important purposes:
1. Reducing computational complexity
2. Controlling overfitting
3. Achieving translation invariance (recognizing patterns regardless of their position)

The two most common pooling methods are:
- Max Pooling: Selects the maximum value from each region of the feature map
- Average Pooling: Calculates the average value of each region

<p style="text-align: center;">
    <img src="/assets/images/2_pooling.png" alt="CNN architecture" width="60%" class="img-center">
</p>


### Flattening

After several rounds of convolution and pooling, the resulting feature maps are flattened into a one-dimensional vector. This transformation prepares the data for processing by fully connected layers

### Fully Connected Layers

Fully connected layers take the flattened input and compute the final classification or regression task. In these layers, each input is connected to every neuron, similar to traditional neural networks

### Output Layer

The output layer provides the final prediction, typically using a Softmax function for classification tasks, which converts the output into probability scores for each class.

---

## CNN architecture LeNet & AlexNet

<p style="text-align: center;">
    <img src="/assets/images/2_cnn_arch.png" alt="CNN architecture" width="60%" class="img-center">
</p>

---