---
layout: notes
title: "Attention Is All You Need"
tags: [ML, AI]
date: 2025-05-10
---

## What is a Attention?
Till now we've looked into RNN encoder-decoder architecture, in which we take the input sequence and an input and we we pass through th encoder we're left with the context vector $c$, and the initial decoder state $s_0$. Then we take this initial state $d_0$, $c$ and $[START]$ token and pass it through the decoder, the output generated is again taken as input in the next step and we continue to generate until it generates a special token $[STOP]$. This works fine as long as the input sentences are short. If the input sentence becomes long the vectors $c$, $s_0$ were not able to $summarize$ the whole context. We could have compressed each sentence in a paragraph and then send it to decoder, but due to sequential nature of RNN this method won't really scale up.  The intuition behind attention is that rather than compressing the input, it might be better for the decoder to revisit the input sequence at every step. Attention solved this by allowing models to focus on different parts of the input when generating each element of the output. Basically the decoder selectively focus on particular parts of the input sequence at particular decoding steps. Intituively let's try to understand attention. Let's say we have a sentence "The cat sat on the mat". When we want to generate the word "sat", we should focus on the words "cat" and "on", while when generating the word "mat", we should focus on the words "the" and "sat". The attention mechanism allows the model to compute attention scores between each word in the sentence, enabling it to capture relationships between words. For example, the word "cat" may have a high attention score with "sat" and "mat", indicating that these words are related in the context of the sentence.

The concept of attention draws inspiration from human cognition - just as we focus on specific parts of visual or auditory input when processing information, neural networks with attention mechanisms can selectively focus on relevant parts of input data when performing tasks.

Mathematically, attention creates a direct pathway between decoder steps and all encoder hidden states. For input sequence $ \{h_1, h_2, ..., h_T\} $ and decoder state $ s_t $, the context vector $ c_t $ becomes:

$$
c_t = \sum_{i=1}^T \alpha_{t,i}h_i
$$

where $ \alpha_{t,i} $ represents attention weights calculated through learned alignment models. This fundamental innovation eliminated information bottlenecks while enabling models to handle longer sequences effectively.

---

## Q,K,V - Queries, Keys, Values

The query-key-value (QKV) framework forms the mathematical foundation of modern attention mechanisms. These three vectors create an information retrieval system within neural networks:

1. **Query (Q):** Represents the current focus of attention (e.g., decoder state in translation)
2. **Key (K):** Encodes characteristics of available information (e.g., encoder hidden states)
3. **Value (V):** Contains actual information to be aggregated

Let's take a simple example data $ D = {(k_1, v_1), (k_2, v_2), ... (k_n, v_n)} $ where $ k_i $ is the key and $ v_i $ is the value. The attention mechanism computes a weighted sum of values based on the similarity between the query and keys. The weights are determined by a scoring function that measures how well each key matches the query. Intituively let's try to understand the QKV framework. Let's say we have a sentence "The cat sat on the mat". The query could be the word "sat", while the keys could be all the words in the sentence, and the values could be the corresponding embeddings of those words. The attention mechanism computes attention scores between the query and keys, enabling it to capture relationships between words. For example, the word "sat" may have a high attention score with "cat" and "on", indicating that these words are related in the context of the sentence.
For example, if we have a query $ q $ and keys $ k_1, k_2, ..., k_n $, the attention score for each key can be computed as:

$$
\text{score}(q, k_i) = \text{similarity}(q, k_i)
$$

where similarity can be computed using various methods, such as dot product, cosine similarity, or learned functions. The attention weights are then obtained by applying a softmax function to the scores:

$$
\alpha_i = \frac{\exp(\text{score}(q, k_i))}{\sum_{j=1}^n \exp(\text{score}(q, k_j))}
$$

The final output of the attention mechanism is a weighted sum of the values:

$$
\text{Attention}(q, D) = \sum_{i=1}^n \alpha_i v_i
$$

This means that the output is a combination of the values, where the weights are determined by how well the query matches each key. The attention mechanism allows the model to focus on different parts of the input sequence based on the current query, enabling it to capture relevant information dynamically.
In the context of neural networks, the QKV framework is often implemented using learned linear transformations. For example, given an input sequence $ X = [x_1, x_2, ..., x_n] $, we can compute the queries, keys, and values as follows:

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$

where $ W^Q, W^K, W^V $ are learned weight matrices. The attention mechanism can then be expressed as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

where $ d_k $ is the dimensionality of the keys. The softmax function ensures that the attention weights sum to 1, allowing the model to focus on the most relevant parts of the input sequence.
The attention mechanism can be visualized as a weighted sum of the values, where the weights are determined by the similarity between the query and keys. This allows the model to dynamically focus on different parts of the input sequence based on the current query, enabling it to capture relevant information effectively.

---

## Bahdanau Attention
Bahdanau attention, also known as additive attention, was introduced in the paper "Neural Machine Translation by Jointly Learning to Align and Translate" by Bahdanau et al. (2014). It uses a feedforward neural network to compute alignment scores between the decoder state and encoder hidden states. The attention weights are computed using a single-layer feedforward network with a tanh activation function. 
The attention mechanism can be expressed as:

$$
\begin{align*}
\text{score}(s_t, h_i) &= \text{tanh}(W_a[s_t; h_i] + b_a) \\
\alpha_{t,i} &= \text{softmax}(\text{score}(s_t, h_i)) \\
c_t &= \sum_{i=1}^T \alpha_{t,i} h_i
\end{align*}
$$

where $W_a$ and $b_a$ are learned parameters, and $[s_t; h_i]$ denotes the concatenation of the decoder state and encoder hidden state. The attention weights $\alpha_{t,i}$ are computed using the softmax function, and the context vector $c_t$ is a weighted sum of the encoder hidden states.

---

## Luong Attention
Luong attention, also known as multiplicative attention, was introduced in the paper "Effective Approaches to Attention-based Neural Machine Translation" by Luong et al. (2015). It uses a dot product to compute alignment scores between the decoder state and encoder hidden states. The attention weights are computed using a softmax function applied to the alignment scores.
The attention mechanism can be expressed as:

$$
\begin{align*}
\text{score}(s_t, h_i) &= s_t^T W_a h_i \\
\alpha_{t,i} &= \text{softmax}(\text{score}(s_t, h_i)) \\
c_t &= \sum_{i=1}^T \alpha_{t,i} h_i
\end{align*}
$$

where $W_a$ is a learned parameter matrix. The attention weights $\alpha_{t,i}$ are computed using the softmax function, and the context vector $c_t$ is a weighted sum of the encoder hidden states.

---

## Scaled Dot-Product Attention

<p style="text-align: center;">
    <img src="/assets/images/4_dot_att.png" width="25%" class="img-center">
</p>

Scaled dot-product attention is a variant of the dot-product attention mechanism that scales the alignment scores by the square root of the dimensionality of the keys. This scaling helps to prevent large values in the softmax function, which can lead to numerical instability.
The attention mechanism can be expressed as:

$$
\begin{align*}
\text{score}(Q, K) &= \frac{QK^T}{\sqrt{d_k}} \\
\alpha &= \text{softmax}(\text{score}(Q, K)) \\
c &= \alpha V
\end{align*}
$$

where $d_k$ is the dimensionality of the keys. The attention weights $\alpha$ are computed using the softmax function, and the context vector $c$ is a weighted sum of the values.

---

## Multi-Head Attention

<p style="text-align: center;">
    <img src="/assets/images/4_multi_att.png" width="40%" class="img-center">
</p>

Multi-head attention is an extension of the scaled dot-product attention mechanism that allows the model to jointly attend to information from different representation subspaces. It does this by applying multiple attention heads in parallel, each with its own set of learned parameters. Intituively let's try to understand multi-head attention. Let's say we have a sentence "The cat sat on the mat". The multi-head attention mechanism allows the model to compute multiple sets of attention scores between each word in the sentence, enabling it to capture different relationships between words. For example, one attention head may focus on the relationship between "cat" and "sat", while another may focus on the relationship between "sat" and "mat". This allows the model to capture a richer set of relationships between words in the sentence.
The attention mechanism can be expressed as:

$$
\begin{align*}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h) W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}
$$

where $h$ is the number of attention heads, and $W_i^Q$, $W_i^K$, $W_i^V$, and $W^O$ are learned weight matrices. Each attention head computes its own set of attention weights and context vectors, which are then concatenated and linearly transformed to produce the final output.

---

## Self-Attention
Self-attention is a special case of the attention mechanism where the queries, keys, and values all come from the same input sequence. This allows the model to capture relationships between different elements in the sequence without relying on an external context. Intituively let's try to understand self-attention. Let's say we have a sentence "The cat sat on the mat". The self-attention mechanism allows the model to compute attention scores between each word in the sentence, enabling it to capture relationships between words. For example, the word "cat" may have a high attention score with "sat" and "mat", indicating that these words are related in the context of the sentence.
The attention mechanism can be expressed as:

$$
\begin{align*}
\text{score}(Q, K) &= \frac{QK^T}{\sqrt{d_k}} \\
\alpha &= \text{softmax}(\text{score}(Q, K)) \\
c &= \alpha V
\end{align*}
$$

where $Q$, $K$, and $V$ are all derived from the same input sequence. The attention weights $\alpha$ are computed using the softmax function, and the context vector $c$ is a weighted sum of the values.

---

## Positional Encoding
Positional encoding is a technique used to inject information about the position of elements in a sequence into the input embeddings. Since the attention mechanism does not inherently capture positional information, positional encodings are added to the input embeddings to provide this context. Intituively let's try to understand positional encoding. In a sentence, the order of words matters. For example, "The cat sat on the mat" has a different meaning than "The mat sat on the cat". Positional encoding helps the model understand the order of words in a sentence by adding unique positional information to each word's embedding.
The positional encoding can be expressed as:

$$
\begin{align*}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)
\end{align*}
$$

where $pos$ is the position of the element in the sequence, $i$ is the dimension index, and $d_{model}$ is the dimensionality of the model. The positional encodings are added to the input embeddings to provide information about the relative positions of elements in the sequence.

---

## Transformer

<p style="text-align: center;">
    <img src="/assets/images/4_transformer.png" width="50%" class="img-center">
</p>

The Transformer architecture is a neural network model that relies entirely on attention mechanisms to draw global dependencies between input and output. It consists of an encoder and decoder, each composed of multiple layers of multi-head self-attention and feedforward neural networks. The Transformer architecture has been shown to achieve state-of-the-art performance in various natural language processing tasks, such as machine translation and text generation.

### Encoder
The encoder is a stack of $N$ identical layers, each consisting of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feedforward network. The encoder takes an input sequence and generates a set of continuous representations that capture the relationships between the elements in the sequence.
The encoder can be expressed as:

$$
\begin{align*}
\text{Encoder}(X) &= \text{LayerNorm}(Z + \text{FeedForward}(Z)) \\
\text{Z} &= \text{LayerNorm}(X + \text{MultiHeadSelfAttention}(X, X, X)) \\
\end{align*}
$$


where $X$ is the input embeddings (with positional encoding added beforehand), and $\text{FeedForward}(Z)$ is a position-wise feedforward network applied to each position in the sequence. The LayerNorm operation normalizes the output of each sub-layer, and the PositionalEncoding function adds positional information to the input embeddings.

### Decoder
The decoder is also a stack of $N$ identical layers, but each layer consists of three sub-layers: a multi-head self-attention mechanism, a multi-head attention mechanism that attends to the encoder's output, and a position-wise fully connected feedforward network. The decoder takes the encoder's output and generates an output sequence.
The decoder can be expressed as:

$$
\begin{align*}
\text{Decoder}(Y) &= \text{LayerNorm}(Z_2 + \text{FeedForward}(Z_2)) \\
{Z_2} &= \text{LayerNorm}(Z_1 + \text{MultiHeadSelfAttention}(Z_1, \text{EncOut}, \text{EncOut})) \\
{Z_1} &= \text{LayerNorm}(Y + \text{MultiHeadSelfAttention}(Y, Y, Y)) \\
\end{align*}
$$

where $Y$ is the input embeddings (with positional encoding added beforehand), $\text{EncOut}$ is the output of the encoder, and $\text{FeedForward}(Z_2)$ is a position-wise feedforward network applied to each position in the sequence. The LayerNorm operation normalizes the output of each sub-layer, and the PositionalEncoding function adds positional information to the input embeddings.


### Final Linear and Softmax Layer
The final linear and softmax layer is applied to the output of the decoder to generate the final predictions. The output of the decoder is passed through a linear layer followed by a softmax function to produce a probability distribution over the vocabulary.
The final output can be expressed as:

$$
\begin{align*}
\text{Output}(Y) &= \text{softmax}(W_o^T \cdot \text{Decoder(Y)} + b_o) \\
\end{align*}
$$

where $W^o$ is a learned weight matrix. The softmax function produces a probability distribution over the vocabulary, allowing the model to generate the final output sequence.

---