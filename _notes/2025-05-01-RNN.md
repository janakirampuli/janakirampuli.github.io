---
layout: notes
title: "Sequential Modeling - Recurrent Neural Networks"
tags: [ML, AI]
date: 2025-05-01
---

## What is a Sequence Model?

A simple explanation is that the sequence models are the machine learning models that input or output sequences of data. Sequential data includes text streams, audio clips, video clips, time-series data and etc. What are the advantage we're getting with these sequential models? In understanding the time-dependent data. The main challenge these models are addressing is the learning from data where order matters. Unlike traditional machine learning approaches that treat each data point as independent, sequence models recognize the inherent connections between elements in a sequence, capturing temporal dependencies and patterns that unfold over time.  Neural networks struggle with sequential data due to their assumption that inputs and outputs are independent of one another. These networks expect fixed-size inputs and cannot effectively account for the order of elements in a sequence.

---

## Recurrent Neural Networks (RNN)

Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. Let's try to undetstand what this means. RNNs are simply a feed-forward network that has an **internal memory** that helps in predicting the next thing in sequence. This memory feature is obtained due to the recurrent nature of RNNs, where it utilizes a hidden state to gather context about the sequence given as input. The fundamental building block of RNNs is the recurrent unit, which maintains a hidden state - a form of memory that is updated at each time step based on the current input and the previous hidden state. This feedback mechanism allows the network to learn from past inputs and incorporate that knowledge into it's current processing. Unfolding a RNN looks like this:

<p style="text-align: center;">
    <img src="/assets/images/3_rnn1.png" alt="RNN architecture" width="60%" class="img-center">
</p>

For an input sequence $ X = (x^{\lt 1 \gt}, x^{\lt 2 \gt}, ..., x^{\lt T_x \gt}) $, the RNN computes hidden states $ a^{\lt t \gt} $ and outputs $ y^{\lt t \gt} $ through recursive applications:

$$
a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)
$$

$$
y^{<t>} = g(W_{ya}a^{<t>} + b_y)
$$

Where:

- $ W_{aa} \in \mathbb{R}^{n_a \times n_a} $: Recurrent weight matrix
- $ W_{ax} \in \mathbb{R}^{n_a \times n_x} $: Input projection matrix
- $ W_{ya} \in \mathbb{R}^{n_y \times n_a} $: Output transformation matrix
- $ g $: Activation function

<p style="text-align: center;">
    <img src="/assets/images/3_rnn2.png" width="60%" class="img-center">
</p>

Loss Function is summation of loss at each time step

$$
\mathcal{L} ( \hat{y}, y ) = \sum_{t=1}^{T_y} \mathcal{L}( \hat{y}^{<t>}, y^{<t>} )
$$

The gradient computation unfolds the network across time steps and applies chain rule differentiation(Backpropagation through time):

$$
\frac{\partial \mathcal{L}^{(T)}}{\partial W} = \sum_{t=1}^T {\frac{\partial \mathcal{L}^{(T)}}{\partial W}} \vert_{(t)}
$$

$$
\frac{\partial \mathcal{L}^{(T)}}{\partial W} = \sum_{t=1}^T \frac{\partial \mathcal{L}}{\partial y^{<t>}} \frac{\partial y^{<t>}}{\partial a^{<t>}} \left( \sum_{k=1}^t \frac{\partial a^{<t>}}{\partial a^{<k>}} \frac{\partial a^{<k>}}{\partial W} \right)
$$

Trying to understand this - 

"**Vanishing/exploding gradient**: The vanishing and exploding gradient phenomena are often encountered in the context of RNNs. The reason why they happen is that it is difficult to capture long term dependencies because of multiplicative gradient that can be exponentially decreasing/increasing with respect to the number of layers."

The term $\frac{\partial a^{\lt t \gt}}{\partial a^{\lt k \gt}}$, creates multiplicative interactions of weight matrices across time intervals, leading to gradient instability...

---

## Advanced Gated Architectures

### Long Short-Term Memory (LSTM)

<p style="text-align: center;">
    <img src="/assets/images/3_LSTM_Cell.svg" width="60%" class="img-center">
</p>

The term "Long Short-Term Memory" comes from the following intuition. Simple recurrent neural networks have long-term memory in the form of weights. The weights change slowly during training, encoding general knowledge about the data. They also have short-term memory in the form of ephemeral activations, which pass from each node to successive nodes. This happens via the gates that determine whether 
- A given input should impact the internal state (the input gate)
- The internal state should be flushed to 
 (the forget gate)
- The internal state of a given neuron should be allowed to impact the cell's output (the output gate).

**Gate Definitions**

**Forget Gate**:

$$
\Gamma_f^{<t>} = \sigma(W_f[h^{<t-1>}, x^{<t>}] + b_f)
$$

**Input Gate**:

$$
\Gamma_i^{<t>} = \sigma(W_i[h^{<t-1>}, x^{<t>}] + b_i)
$$

**Output Gate**:

$$
\Gamma_o^{<t>} = \sigma(W_o[h^{<t-1>}, x^{<t>}] + b_o)
$$

**State Update Equations**

$$
\tilde{c}^{<t>} = \tanh(W_c[h^{<t-1>}, x^{<t>}] + b_c)
$$

$$
c^{<t>} = \Gamma_f^{<t>} \odot c^{<t-1>} + \Gamma_i^{<t>} \odot \tilde{c}^{<t>}
$$

$$
h^{<t>} = \Gamma_o^{<t>} \odot \tanh(c^{<t>})
$$

The cell state $ c^{\lt t \gt} $(context) creates a linear data highway that preserves gradient magnitude across extended sequences.

---

### Gated Recurrent Unit (GRU)

<p style="text-align: center;">
    <img src="/assets/images/3_gru.svg" width="60%" class="img-center">
</p>

LSTM's three gates are replaced by two: the reset gate and the update gate. Intuitively, the reset gate controls how much of the previous state we might still want to remember. Update gate would allow us to control how much of the new state is just a copy of the old one.
GRUs condense the gating mechanism into two components while maintaining effectiveness:


**Gate Definitions**

**Update and Reset Gates**

$$
\Gamma_u^{<t>} = \sigma(W_u[h^{<t-1>}, x^{<t>}] + b_u)
$$

$$
\Gamma_r^{<t>} = \sigma(W_r[h^{<t-1>}, x^{<t>}] + b_r)
$$

**Hidden State Computation**

$$
\tilde{h}^{<t>} = \tanh(W[\Gamma_r^{<t>} \odot h^{<t-1>}, x^{<t>}] + b)
$$

$$
h^{<t>} = \Gamma_u^{<t>} \odot \tilde{h}^{<t>} + (1 - \Gamma_u^{<t>}) \odot h^{<t-1>}
$$

---

## Types of Recurrent Neural Networks

RNNs can be categorized based on their input-output relationships, with each type suited for specific applications:

### One-to-One

The simplest RNN architecture accepts a single input and produces a single output. This configuration functions similarly to a standard neural network with fixed input and output sizes.

**Application**: Image classification, where a single image produces a single class label.

### One-to-Many

This architecture takes a single input and generates a sequence of outputs. The input size is fixed, but it produces a series of data outputs.

**Applications**: Music generation (a genre label generates a sequence of notes), Image captioning (an image generates a sequence of words)


### Many-to-One

This configuration processes a sequence of inputs to produce a single output. It converges a sequence into a unified representation through a series of hidden layers that learn features from the sequence.

**Application**: Sentiment analysis, where a sequence of words determines a single sentiment label.

### Many-to-Many

This architecture handles sequences for both input and output. It comes in two variations:

1. **Equal Size**: The input and output sequences have identical length.
**Application**: Part-of-speech tagging, where each word receives a grammatical tag.
2. **Unequal Size**: The input and output sequences have different lengths.
**Application**: Machine translation, where sentences in different languages often have varying lengths.

---

### Stacked RNN

If we stack the RNNs on top of each other we have a stacked RNN. If the input is of length N and the first RNN produces the sequence of outputs of length N. Then these are the inputs to the next RNN layer. 

<p style="text-align: center;">
    <img src="/assets/images/3_stacked_RNN.png" width="60%" class="img-center">
</p>

---

### Bidirectional RNN

Bidirectional RNNs is an advancement in sequential data processing by enabling networks to capture both past and future contextual information. Unlike traditional unidirectional RNNs that process sequences in a single temporal direction, BRNNs employ two parallel RNN layers operating in opposite directions. Forward RNN processes input sequence from $t=1$ to $t=N$. Backward RNN processes input sequence from $t=T$ to $t=1$.

<p style="text-align: center;">
    <img src="/assets/images/3_bi_RNN.png" width="60%" class="img-center">
</p>

---

### Encoder-Decoder Architecture


In general sequence-to-sequence learning like machine transalation, inputs and outputs are of varying lengths and unaligned.

1. Encoder: Processes the input sequence and transforms it into a different representation, typically a fixed-dimensional vector (also called the "context vector"). This vector aims to capture the semantic meaning of the entire input sequence.

2. Decoder: Takes the encoded representation and generates the output sequence, one element at a time. The decoder essentially functions as a language model conditioned on the encoded input.

<p style="text-align: center;">
    <img src="/assets/images/3_en_de.png" width="60%" class="img-center">
</p>

This design allows the model to handle variable-length sequences by compressing all necessary information into the fixed-dimensional intermediate representation

Why It Works? - The encoder-decoder architecture is powerful because it makes minimal assumptions about the data structure. It can learn complex relationships between input and output sequences through end-to-end training. The use of LSTMs is particularly important because they excel at capturing long-range dependencies in sequences - a critical requirement for tasks like translation.

---

## Sequence to Sequence Learning with Neural Networks [<a>[Paper](https://arxiv.org/pdf/1409.3215)</a>]

Let's understand the groundbreaking 2014 paper "Sequence to Sequence Learning with Neural Networks" and how this work revolutionized machine translation and laid the foundation for modern NLP. The paper introduced a powerful end-to-end approach using Long Short-Term Memory (LSTM) networks that outperformed traditional statistical machine translation systems while making minimal assumptions about sequence structure.

### Introduction to Sequence-to-Sequence Learning
Sequence-to-sequence (seq2seq) learning addresses a fundamental challenge in machine learning: how to map an input sequence to an output sequence when both can have different and variable lengths. Before 2014, this was a significant limitation for deep neural networks (DNNs), which typically required fixed-dimensional inputs and outputs.

The challenge is clear in tasks like machine translation, where input and output sequences (sentences in different languages) vary in length and have complex relationships. Traditional DNNs couldn't handle this effectively, but seq2seq models offered an elegant solution by transforming the problem into two parts: encoding the input sequence into a fixed-dimensional representation, and then decoding that representation into the target sequence.

### The Seq2Seq Paper

In their 2014 paper, they presented a neural network architecture that could "learn to map sequences to sequences". Their approach used two separate Long Short-Term Memory (LSTM) networks:

- An encoder LSTM reads the entire input sequence and produces a fixed-dimensional vector
- A decoder LSTM generates the output sequence from this vector

<p style="text-align: center;">
    <img src="/assets/images/3_seq2seq.png" width="60%" class="img-center">
</p>

Their model achieved remarkable results on English-to-French translation tasks from the WMT'14 dataset, attaining a BLEU score of 34.8 - outperforming the phrase-based Statistical Machine Translation (SMT) baseline which scored 33.3.

The model aims to maximize the conditional probability:

$$
p(y_1, \ldots, y_{T'} | x_1, \ldots, x_T) = \prod_{t=1}^{T'} p(y_t | c, y_1, \ldots, y_{t-1})
$$

where $c$ is the fixed-dimensional context vector from the encoder

---

### Components of Seq2Seq Models

### The Encoder Component

The encoder's job is to process the input sequence and create a meaningful numerical representation that captures its essence. In this paper, the encoder is a multilayered LSTM network.

The encoder reads the input sequence one token at a time, updating its hidden state at each step. After processing the entire sequence, the final hidden state serves as the fixed-dimensional representation (the context vector) that encapsulates the input sequence's information.

Interestingly, the paper found that reversing the order of words in the input sequence significantly improved performance. This technique helped create more direct connections (short-term dependencies) between the source and target sequences, making the optimization problem easier.

### The Decoder Component

The decoder takes the context vector produced by the encoder and generates the output sequence token by token. Like the encoder, the decoder in the original paper was also implemented as a multilayered LSTM network.

The decoding process starts with a special "start-of-sequence" token and the context vector from the encoder. At each step, the decoder:

1. Takes the previous token and current hidden state as input
2. Produces a probability distribution over the possible next tokens
3. Selects the most likely token as the next output
4. Updates its hidden state
5. Repeats until a special "end-of-sequence" token is generated

This process allows the model to generate variable-length output sequences, making it suitable for tasks like translation where output length cannot be predetermined.

---

### Training Sequence-to-Sequence Models

### Teacher Forcing

Training seq2seq models presents a unique challenge: how do we handle the dependency between outputs at different time steps? The solution is a technique called teacher forcing.

Teacher forcing is a strategy where, during training, the model receives the actual target output from the previous time step as input for the current step, rather than using its own prediction. In other words:

- During normal inference, the decoder would use its own generated token from step t-1 as input for step t
- With teacher forcing, it instead uses the ground truth token from step t-1 as input for step t
This approach helps the model learn more effectively by:
- Reducing the impact of error accumulation during training
- Providing more stable gradients
- Accelerating convergence

However, teacher forcing creates a discrepancy between training and inference - the model is trained with perfect previous tokens but must use its own predictions during inference. This can lead to error accumulation during inference.

### Training Details from the Original Paper
The training approach described in the paper includes several important details:

1. They used deep LSTMs with 4 layers, 1000 cells at each layer, and 1000-dimensional word embeddings
2. Input vocabulary size was 160,000 and output vocabulary size was 80,000
3. Parameters were initialized uniformly between -0.08 and 0.08
4. They used stochastic gradient descent without momentum, with a learning rate of 0.7
5. After 5 epochs, they halved the learning rate every half epoch, training for a total of 7.5 epochs
6. They enforced a hard constraint on gradient norms by scaling when the norm exceeded a threshold

Sentences in a minibatch were of similar length to improve computational efficiency

---

### Prediction and Inference in Seq2Seq Models

### Greedy Decoding vs. Beam Search

During inference, the seq2seq model needs to generate the most likely output sequence given an input. Two main approaches exist:

1. Greedy Decoding: Select the token with the highest probability at each time step. This is simple but can lead to suboptimal sequences, as a locally optimal choice might lead to globally suboptimal results.

2. Beam Search: Maintain multiple "partial hypotheses" (potential output sequences) at each step. The beam width B determines how many hypotheses to keep. Beam search provides a trade-off between accuracy and computational cost.

Given context vector $ v $, beam search maintains $ B $ hypotheses with scores:

$$
s(y_{1:t}) = \sum_{i=1}^t \log p(y_i | y_{1:i-1}, v)
$$

At each step:
1. Expand each hypothesis to $ V $ candidates
2. Keep top-$ B $ by accumulated score
3. Stop when EOS generated or max length reached

With beam size $ B=2 $, the paper achieved 2.8 BLEU improvement over greedy decoding.


### How Beam Search Works

1. Start with a single hypothesis containing just the start-of-sequence token
2. For each existing hypothesis, generate B×V potential new hypotheses by appending each token in the vocabulary (of size V)
3. Score each new hypothesis using the model's probability
4. Keep only the B highest-scoring new hypotheses
5. Continue until the end-of-sequence token is generated or a maximum length is reached

The original paper found that even with a small beam size (B=2), beam search significantly improved translation quality compared to greedy decoding.

---

### Evaluation Metrics: Understanding BLEU Score

### What is BLEU?

BLEU (Bilingual Evaluation Understudy) measures the correspondence between a machine's output and human reference translations.

The core idea behind BLEU is simple: a good translation should share many of the same words and phrases with professional human translations. BLEU calculates scores by comparing n-grams (sequences of n consecutive words) in the candidate translation with n-grams in the reference translations and counting the matches.

### How BLEU is Calculated
The calculation involves:

1. Precision: The percentage of n-grams in the candidate translation that appear in any reference translation
2. Brevity Penalty: A factor that penalizes translations that are too short
3. Geometric Mean: Combines scores from different n-gram sizes (typically unigrams, bigrams, trigrams, and 4-grams)

The formula ensures that translations must match reference translations in both word choice and word order to achieve a high score.

The BLEU metric combines:

- **Modified n-gram Precision**:

$$
p_n = \frac{\sum_{C\in\{Candidates\}} \sum_{n\text{-gram}\in C} \text{Count}_{\text{clip}}(n\text{-gram})}{\sum_{C'\in\{Candidates\}} \sum_{n\text{-gram}'\in C'} \text{Count}(n\text{-gram}')}
$$

- **Brevity Penalty**:

$$
BP = \begin{cases}
1 & \text{if } c \gt r \\
e^{1-r/c} & \text{if } c \leq r
\end{cases}
$$

- **Composite Score**:

$$
BLEU = BP \cdot \exp\left(\sum_{n=1}^N w_n \log p_n\right)
$$

Where $ c $ = candidate length, $ r $ = reference length, $w_n$ is the weight applied to the n-gram precision score ($p_n$). It's often set to $1/n$, where $n$ is the number of n-gram sizes used.


BLEU in the Original Paper
In this paper, their best model achieved a BLEU score of 34.8 on the WMT'14 English-to-French test set. This was a significant achievement as it outperformed the phrase-based SMT baseline (33.3). When they used their LSTM to rescore the 1000-best lists produced by the SMT system, the BLEU score increased to 36.5.

---


